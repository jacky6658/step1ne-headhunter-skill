#!/bin/bash
# å¡«è£œ BDå®¢æˆ¶é–‹ç™¼è¡¨ä¸­ç¼ºå°‘çš„è¯çµ¡è³‡è¨Š

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SHEET_ID="1bkI7_cCh_Bs4qVa3HlXiy0CFzmItZlA-DYGHSPS4InE"
GOG_ACCOUNT="aiagentg888@gmail.com"

# é¡è‰²
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}ðŸ“‹ è®€å– BDå®¢æˆ¶é–‹ç™¼è¡¨...${NC}"

# è®€å–æ•´å€‹è¡¨æ ¼ï¼ˆJSON æ ¼å¼ï¼‰
gog sheets get "$SHEET_ID" 'å·¥ä½œè¡¨1!A:E' --json --account "$GOG_ACCOUNT" > /tmp/bd-raw.json

# æå–éœ€è¦è™•ç†çš„å…¬å¸ï¼ˆè¯çµ¡é›»è©±=å¾…æŸ¥ ä¸” æœ‰104å…¬å¸URLï¼‰
echo -e "${BLUE}ðŸ” æ‰¾å‡ºéœ€è¦è£œå……è¯çµ¡è³‡è¨Šçš„å…¬å¸...${NC}"

cat > /tmp/process-bd-companies.py << 'EOF'
#!/usr/bin/env python3
import sys
import json

# è®€å– JSON
with open('/tmp/bd-raw.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

companies = []

# data æ ¼å¼ï¼š{"range": ..., "values": [[row1], [row2], ...]}
rows = data.get('values', [])

for i, row in enumerate(rows[1:], 2):  # è·³éŽæ¨™é¡Œè¡Œï¼Œå¾žç¬¬2è¡Œé–‹å§‹
    if len(row) >= 5:
        company_name = row[0].strip() if row[0] else ""
        phone = row[1].strip() if len(row) > 1 and row[1] else ""
        email = row[2].strip() if len(row) > 2 and row[2] else ""
        website = row[3].strip() if len(row) > 3 and row[3] else ""
        company_url = row[4].strip() if len(row) > 4 and row[4] else ""
        
        # åªè™•ç†ï¼šæœ‰å…¬å¸URL ä¸” (é›»è©±=å¾…æŸ¥ æˆ– email=å¾…æŸ¥)
        if company_url and company_url.startswith('https://www.104.com.tw/company/'):
            if phone == "å¾…æŸ¥" or email == "å¾…æŸ¥" or not phone or not email:
                companies.append({
                    "row": i,
                    "company": company_name,
                    "company_url": company_url,
                    "current_phone": phone,
                    "current_email": email,
                    "current_website": website
                })

print(json.dumps(companies, ensure_ascii=False, indent=2))
EOF

python3 /tmp/process-bd-companies.py > /tmp/companies-to-process.json

count=$(cat /tmp/companies-to-process.json | jq '. | length')
echo -e "${GREEN}âœ… æ‰¾åˆ° ${count} å®¶å…¬å¸éœ€è¦è£œå……è¯çµ¡è³‡è¨Š${NC}"

if [[ $count -eq 0 ]]; then
    echo "æ²’æœ‰éœ€è¦è™•ç†çš„å…¬å¸"
    exit 0
fi

# é¡¯ç¤ºè¦è™•ç†çš„å…¬å¸
echo -e "${YELLOW}è¦è™•ç†çš„å…¬å¸ï¼ˆå…± ${count} å®¶ï¼‰ï¼š${NC}"
cat /tmp/companies-to-process.json | jq -r '.[] | "  - \(.company) (Row \(.row))"'
echo ""

# å»ºç«‹çˆ¬èŸ²è…³æœ¬
cat > /tmp/scrape-bd-contacts.py << 'EOFPY'
#!/usr/bin/env python3
import subprocess
import re
import json
import sys
import time
import random
from datetime import datetime

# è«‹æ±‚é »çŽ‡é™åˆ¶ï¼šæ¯å°æ™‚ â‰¤30 æ¬¡ = æ¯æ¬¡é–“éš” â‰¥120 ç§’
REQUEST_INTERVAL = 120  # ç§’

def log_debug(message):
    with open("/tmp/bd-contact-fill-debug.log", 'a', encoding='utf-8') as log:
        log.write(f"[{datetime.now()}] {message}\n")

def extract_company_contact(company_url):
    """å¾žå…¬å¸é é¢æå–è¯çµ¡è³‡è¨Š"""
    log_debug(f"ðŸ“ž è¨ªå•: {company_url}")
    
    subprocess.run(['agent-browser', 'navigate', company_url], check=True, capture_output=True)
    subprocess.run(['agent-browser', 'wait', '--load', 'networkidle', '--timeout', '10000'], check=True, capture_output=True)
    
    # éš¨æ©Ÿå»¶é² 2-5 ç§’ï¼ˆé˜²åçˆ¬èŸ²ï¼‰
    delay = random.uniform(2, 5)
    log_debug(f"  â³ å»¶é² {delay:.1f} ç§’")
    time.sleep(delay)
    
    result = subprocess.run(['agent-browser', 'snapshot'], capture_output=True, text=True)
    lines = result.stdout.split('\n')
    
    contact_info = {
        "contact_person": None,
        "phone": None,
        "fax": None,
        "email": None,
        "address": None,
        "website": None
    }
    
    for i, line in enumerate(lines):
        # è¯çµ¡äºº
        if 'heading "è¯çµ¡äºº"' in line and i+1 < len(lines):
            next_line = lines[i+1].strip()
            if '- paragraph:' in next_line:
                name = next_line.replace('- paragraph:', '').strip()
                if name and name != "æš«ä¸æä¾›":
                    contact_info['contact_person'] = name
        
        # é›»è©±
        if 'heading "é›»è©±"' in line and i+1 < len(lines):
            next_line = lines[i+1].strip()
            if '- paragraph:' in next_line:
                phone = next_line.replace('- paragraph:', '').strip()
                if phone and phone != "æš«ä¸æä¾›":
                    contact_info['phone'] = phone
        
        # å‚³çœŸ
        if 'heading "å‚³çœŸ"' in line and i+1 < len(lines):
            next_line = lines[i+1].strip()
            if '- paragraph:' in next_line:
                fax = next_line.replace('- paragraph:', '').strip()
                if fax and fax != "æš«ä¸æä¾›":
                    contact_info['fax'] = fax
        
        # åœ°å€
        if 'heading "åœ°å€"' in line and i+1 < len(lines):
            next_line = lines[i+1].strip()
            if '- paragraph:' in next_line:
                addr = next_line.replace('- paragraph:', '').strip()
                if addr and addr != "æš«ä¸æä¾›":
                    contact_info['address'] = addr
        
        # å…¬å¸ç¶²å€
        if 'heading "å…¬å¸ç¶²å€"' in line and i+2 < len(lines):
            url_line = lines[i+2].strip()
            if '- /url:' in url_line:
                url_match = re.search(r'- /url:\s*(https?://[^\s]+)', url_line)
                if url_match:
                    contact_info['website'] = url_match.group(1).strip()
        
        # Email
        if '@' in line:
            email_match = re.search(r'([\w\.-]+@[\w\.-]+\.\w+)', line)
            if email_match:
                contact_info['email'] = email_match.group(1).strip()
    
    log_debug(f"  é›»è©±: {contact_info['phone']}, Email: {contact_info['email']}")
    return contact_info

def search_email_from_website(website_url):
    """å¾žå…¬å¸å®˜ç¶²æœå°‹ Email"""
    if not website_url or not website_url.startswith('http'):
        return None
    
    log_debug(f"ðŸŒ æœå°‹å®˜ç¶² Email: {website_url}")
    
    contact_pages = [
        website_url,
        f"{website_url.rstrip('/')}/contact",
        f"{website_url.rstrip('/')}/contact-us",
        f"{website_url.rstrip('/')}/about"
    ]
    
    for url in contact_pages:
        try:
            subprocess.run(['agent-browser', 'navigate', url], check=True, capture_output=True, timeout=10)
            subprocess.run(['agent-browser', 'wait', '--load', 'networkidle', '--timeout', '5000'], check=True, capture_output=True, timeout=10)
            
            # éš¨æ©Ÿå»¶é² 2-5 ç§’ï¼ˆé˜²åçˆ¬èŸ²ï¼‰
            delay = random.uniform(2, 5)
            time.sleep(delay)
            
            result = subprocess.run(['agent-browser', 'snapshot'], capture_output=True, text=True, timeout=5)
            
            email_match = re.search(r'([\w\.-]+@[\w\.-]+\.\w+)', result.stdout)
            if email_match:
                email = email_match.group(1)
                log_debug(f"  âœ… æ‰¾åˆ° Email: {email}")
                return email
        except:
            continue
    
    return None

# è®€å–å…¬å¸åˆ—è¡¨
with open('/tmp/companies-to-process.json', 'r', encoding='utf-8') as f:
    companies = json.load(f)

# é–‹å•Ÿç€è¦½å™¨
subprocess.run(['agent-browser', 'open', 'https://www.104.com.tw'], check=True, capture_output=True)
subprocess.run(['agent-browser', 'wait', '--load', 'networkidle', '--timeout', '10000'], check=True, capture_output=True)

log_debug(f"é–‹å§‹è™•ç† {len(companies)} å®¶å…¬å¸")

results = []
processed_count = 0

for company_data in companies:
    company_name = company_data['company']
    company_url = company_data['company_url']
    row = company_data['row']
    
    log_debug(f"\n{'='*60}")
    log_debug(f"ðŸ¢ è™•ç†: {company_name} (Row {row})")
    
    try:
        # å¾žå…¬å¸é é¢æå–è¯çµ¡è³‡è¨Š
        contact_info = extract_company_contact(company_url)
        
        # å¦‚æžœæ²’æœ‰ Emailï¼Œå˜—è©¦å¾žå®˜ç¶²æ‰¾
        if not contact_info['email'] and contact_info['website']:
            contact_info['email'] = search_email_from_website(contact_info['website'])
        
        results.append({
            "row": row,
            "company": company_name,
            "phone": contact_info['phone'] or company_data['current_phone'],
            "email": contact_info['email'] or company_data['current_email'],
            "website": contact_info['website'] or company_data['current_website'],
            "fax": contact_info['fax'] or "",
            "address": contact_info['address'] or "",
            "contact_person": contact_info['contact_person'] or ""
        })
        
        log_debug(f"âœ… å®Œæˆ: {company_name}")
        
        processed_count += 1
        
        # æ¯è™•ç†ä¸€å®¶å…¬å¸ï¼Œç­‰å¾… 120 ç§’ï¼ˆæ¯å°æ™‚ â‰¤30 æ¬¡ï¼‰
        if processed_count < len(companies):
            log_debug(f"â¸ï¸  ç­‰å¾… {REQUEST_INTERVAL} ç§’ï¼ˆé˜²åçˆ¬èŸ²ï¼Œæ¯å°æ™‚ â‰¤30 æ¬¡ï¼‰...")
            time.sleep(REQUEST_INTERVAL)
        
    except Exception as e:
        log_debug(f"âŒ å¤±æ•—: {company_name} - {e}")
        results.append({
            "row": row,
            "company": company_name,
            "phone": company_data['current_phone'],
            "email": company_data['current_email'],
            "website": company_data['current_website']
        })

# é—œé–‰ç€è¦½å™¨
subprocess.run(['agent-browser', 'close'], check=True, capture_output=True)

# è¼¸å‡ºçµæžœ
print(json.dumps(results, ensure_ascii=False, indent=2))

log_debug(f"âœ… å…¨éƒ¨å®Œæˆï¼Œå…±è™•ç† {len(results)} å®¶å…¬å¸")
EOFPY

echo -e "${BLUE}ðŸ•·ï¸  é–‹å§‹çˆ¬å–è¯çµ¡è³‡è¨Š...${NC}"
python3 /tmp/scrape-bd-contacts.py > /tmp/bd-contacts-result.json

# æ›´æ–° Google Sheets
echo -e "${BLUE}ðŸ“ æ›´æ–° Google Sheets...${NC}"

cat > /tmp/update-sheets.py << 'EOFUPDATE'
#!/usr/bin/env python3
import json
import subprocess

with open('/tmp/bd-contacts-result.json', 'r', encoding='utf-8') as f:
    results = json.load(f)

updated = 0
for item in results:
    row = item['row']
    phone = item.get('phone', 'å¾…æŸ¥')
    email = item.get('email', 'å¾…æŸ¥')
    website = item.get('website', '')
    
    # åªæ›´æ–°æœ‰æ–°è³‡è¨Šçš„æ¬„ä½
    if phone and phone != "å¾…æŸ¥":
        # æ›´æ–° B æ¬„ï¼ˆè¯çµ¡é›»è©±ï¼‰
        range_phone = f"å·¥ä½œè¡¨1!B{row}"
        subprocess.run(['gog', 'sheets', 'update', '1bkI7_cCh_Bs4qVa3HlXiy0CFzmItZlA-DYGHSPS4InE', range_phone, phone, '--account', 'aiagentg888@gmail.com'], check=True, capture_output=True)
        updated += 1
    
    if email and email != "å¾…æŸ¥":
        # æ›´æ–° C æ¬„ï¼ˆEmailï¼‰
        range_email = f"å·¥ä½œè¡¨1!C{row}"
        subprocess.run(['gog', 'sheets', 'update', '1bkI7_cCh_Bs4qVa3HlXiy0CFzmItZlA-DYGHSPS4InE', range_email, email, '--account', 'aiagentg888@gmail.com'], check=True, capture_output=True)
        updated += 1
    
    if website and website != "" and website != "æ‰¾ä¸åˆ°":
        # æ›´æ–° D æ¬„ï¼ˆå…¬å¸å®˜ç¶²ï¼‰
        range_website = f"å·¥ä½œè¡¨1!D{row}"
        subprocess.run(['gog', 'sheets', 'update', '1bkI7_cCh_Bs4qVa3HlXiy0CFzmItZlA-DYGHSPS4InE', range_website, website, '--account', 'aiagentg888@gmail.com'], check=True, capture_output=True)
        updated += 1

print(f"Updated {updated} cells")
EOFUPDATE

python3 /tmp/update-sheets.py

echo -e "${GREEN}âœ… å®Œæˆï¼${NC}"
echo -e "${GREEN}æŸ¥çœ‹æ›´æ–°ï¼šhttps://docs.google.com/spreadsheets/d/$SHEET_ID${NC}"
