#!/usr/bin/env python3
"""
104 å…¬å¸è¯çµ¡æ–¹å¼çˆ¬èŸ²
ç”¨é€”ï¼šå¾ 104 å…¬å¸é é¢æˆ–å®˜ç¶²æå–é›»è©±ã€Emailã€ç¶²å€
"""

import subprocess
import json
import re
import sys
from datetime import datetime

def run_browser_command(cmd):
    """åŸ·è¡Œ agent-browser æŒ‡ä»¤"""
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return result.stdout

def extract_company_id_from_url(url):
    """å¾ 104 è·ç¼º URL æå–å…¬å¸ ID"""
    # URL æ ¼å¼ï¼šhttps://www.104.com.tw/job/xxxxx
    # éœ€è¦è¨ªå•è©²é é¢ï¼Œå¾ä¸­æå–å…¬å¸é€£çµ
    match = re.search(r'/job/([a-z0-9]+)', url)
    if match:
        return match.group(1)
    return None

def scrape_104_company_page(job_url):
    """è¨ªå• 104 è·ç¼ºé é¢ï¼Œæå–å…¬å¸è³‡è¨Š"""
    log_debug(f"è¨ªå•è·ç¼ºé é¢: {job_url}")
    
    # é–‹å•Ÿè·ç¼ºé é¢
    run_browser_command(f'agent-browser open "{job_url}"')
    run_browser_command('agent-browser wait --load networkidle')
    
    # å–å¾—å¿«ç…§
    snapshot_json = run_browser_command('agent-browser snapshot -i --json')
    
    try:
        data = json.loads(snapshot_json)
        refs = data.get('data', {}).get('refs', {})
    except:
        log_debug("âŒ ç„¡æ³•è§£æè·ç¼ºé é¢")
        return None
    
    # è§£æå…¬å¸è³‡è¨Š
    company_info = {
        "phone": None,
        "email": None,
        "website": None,
        "company_url": None
    }
    
    for ref_id, ref_data in refs.items():
        name = ref_data.get('name', '')
        role = ref_data.get('role', '')
        href = ref_data.get('href', '')
        
        # é›»è©±è™Ÿç¢¼
        if not company_info['phone']:
            phone_match = re.search(r'(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{3,4})', name)
            if phone_match:
                company_info['phone'] = phone_match.group(1)
        
        # Email
        if not company_info['email']:
            email_match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', name)
            if email_match:
                company_info['email'] = email_match.group(0)
        
        # å…¬å¸é é¢é€£çµ
        if role == 'link' and '/company/' in href:
            company_info['company_url'] = href if href.startswith('http') else f"https://www.104.com.tw{href}"
        
        # å®˜ç¶²é€£çµ
        if role == 'link' and ('å…¬å¸ç¶²å€' in name or 'å®˜æ–¹ç¶²ç«™' in name or 'å®˜ç¶²' in name):
            if href and not href.startswith('/'):
                company_info['website'] = href if href.startswith('http') else f"https://{href}"
    
    return company_info

def scrape_company_website(website_url):
    """çˆ¬å–å…¬å¸å®˜ç¶²ï¼Œæå–è¯çµ¡æ–¹å¼"""
    if not website_url:
        return {"phone": None, "email": None}
    
    log_debug(f"è¨ªå•å®˜ç¶²: {website_url}")
    
    # å˜—è©¦æ‰¾ã€Œè¯çµ¡æˆ‘å€‘ã€é é¢
    contact_urls = [
        website_url,
        f"{website_url.rstrip('/')}/contact",
        f"{website_url.rstrip('/')}/contact-us",
        f"{website_url.rstrip('/')}/about",
        f"{website_url.rstrip('/')}/about-us"
    ]
    
    for url in contact_urls:
        try:
            run_browser_command(f'agent-browser open "{url}"')
            run_browser_command('agent-browser wait --load networkidle --timeout 5000')
            
            snapshot_json = run_browser_command('agent-browser snapshot -i --json')
            data = json.loads(snapshot_json)
            text = data.get('data', {}).get('text', '')
            
            # æå–é›»è©±
            phone_match = re.search(r'(\d{2,4}[-\s]?\d{3,4}[-\s]?\d{3,4})', text)
            phone = phone_match.group(1) if phone_match else None
            
            # æå– Email
            email_match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', text)
            email = email_match.group(0) if email_match else None
            
            if phone or email:
                return {"phone": phone, "email": email}
        except:
            continue
    
    return {"phone": None, "email": None}

def log_debug(message):
    """å¯«å…¥ debug æ—¥èªŒ"""
    with open("/tmp/104-contact-scraper-debug.log", 'a', encoding='utf-8') as log:
        log.write(f"[{datetime.now()}] {message}\n")

def scrape_company_contact(company_data):
    """çˆ¬å–å…¬å¸è¯çµ¡æ–¹å¼ï¼ˆä¸»å‡½æ•¸ï¼‰"""
    job_url = company_data.get('url', '')
    company_name = company_data.get('company', 'Unknown')
    
    log_debug(f"é–‹å§‹çˆ¬å–: {company_name}")
    
    # æ­¥é©Ÿ 1ï¼šå¾ 104 è·ç¼ºé é¢æå–è³‡è¨Š
    company_info = scrape_104_company_page(job_url)
    
    if not company_info:
        log_debug(f"âŒ ç„¡æ³•æå–å…¬å¸è³‡è¨Š: {company_name}")
        return {**company_data, "phone": "å¾…æŸ¥", "email": "å¾…æŸ¥", "website": "å¾…æŸ¥"}
    
    # æ­¥é©Ÿ 2ï¼šå¦‚æœæ²’æœ‰é›»è©±æˆ– Emailï¼Œå˜—è©¦çˆ¬å®˜ç¶²
    if (not company_info['phone'] or not company_info['email']) and company_info['website']:
        log_debug(f"ğŸ“¡ å˜—è©¦çˆ¬å–å®˜ç¶²: {company_info['website']}")
        website_contact = scrape_company_website(company_info['website'])
        
        if not company_info['phone']:
            company_info['phone'] = website_contact['phone']
        if not company_info['email']:
            company_info['email'] = website_contact['email']
    
    # æ•´åˆçµæœ
    result = {
        **company_data,
        "phone": company_info['phone'] or "å¾…æŸ¥",
        "email": company_info['email'] or "å¾…æŸ¥",
        "website": company_info['website'] or "å¾…æŸ¥",
        "contact_person": "æ‚¨å¥½",
        "status": "å¾…è¯ç¹«"
    }
    
    log_debug(f"âœ… å®Œæˆ: {company_name} | Phone: {result['phone']} | Email: {result['email']}")
    return result

def main():
    """ä¸»ç¨‹å¼"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 scraper-company-contact.py <companies.json>", file=sys.stderr)
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    # è®€å–å…¬å¸åˆ—è¡¨
    with open(input_file, 'r', encoding='utf-8') as f:
        companies = json.load(f)
    
    log_debug(f"é–‹å§‹è™•ç† {len(companies)} å®¶å…¬å¸")
    
    # è™•ç†æ¯å®¶å…¬å¸
    detailed_companies = []
    for company in companies:
        detailed = scrape_company_contact(company)
        detailed_companies.append(detailed)
    
    # è¼¸å‡º JSON
    print(json.dumps(detailed_companies, ensure_ascii=False, indent=2))
    
    # é—œé–‰ç€è¦½å™¨
    run_browser_command('agent-browser close')
    
    log_debug(f"âœ… å…¨éƒ¨å®Œæˆï¼Œå…±è™•ç† {len(detailed_companies)} å®¶å…¬å¸")

if __name__ == "__main__":
    main()
