#!/usr/bin/env python3
"""
PTT Soft_Job ç‰ˆçˆ¬èŸ²
çˆ¬å–æŠ€è¡“äººæ‰è‡ªæˆ‘ä»‹ç´¹æ–‡ç« 
"""

import sys
import json
import re
import time
from datetime import datetime
import subprocess

def fetch_ptt_board(board="Soft_Job", pages=5):
    """çˆ¬å– PTT çœ‹æ¿æ–‡ç« åˆ—è¡¨"""
    
    articles = []
    
    for page in range(1, pages + 1):
        # PTT ç¶²é ç‰ˆ URL
        if page == 1:
            url = f"https://www.ptt.cc/bbs/{board}/index.html"
        else:
            # è¨ˆç®—é ç¢¼ï¼ˆPTT æ˜¯å€’åºï¼‰
            url = f"https://www.ptt.cc/bbs/{board}/index{page}.html"
        
        print(f"ğŸ” çˆ¬å– {board} ç¬¬ {page} é ...", file=sys.stderr)
        
        try:
            result = subprocess.run(
                ['curl', '-s', '-b', 'over18=1', url],  # åŠ  cookie ç¹é 18+ è­¦å‘Š
                capture_output=True,
                text=True,
                timeout=10
            )
            
            html = result.stdout
            
            # è§£ææ–‡ç« é€£çµ
            # PTT æ ¼å¼ï¼š<div class="title"><a href="/bbs/Soft_Job/M.xxx.html">æ¨™é¡Œ</a></div>
            pattern = r'<div class="title">\s*<a href="(/bbs/' + board + r'/M\.\d+\.A\.[A-F0-9]+\.html)">(.+?)</a>'
            matches = re.findall(pattern, html)
            
            for link, title in matches:
                articles.append({
                    'title': title.strip(),
                    'url': f"https://www.ptt.cc{link}",
                    'board': board
                })
            
            time.sleep(1)  # é¿å…éå¿«
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±æ•—ï¼š{e}", file=sys.stderr)
    
    return articles

def parse_article_content(url):
    """è§£ææ–‡ç« å…§å®¹"""
    
    try:
        result = subprocess.run(
            ['curl', '-s', '-b', 'over18=1', url],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        html = result.stdout
        
        # æå–æ–‡ç« å…§å®¹ï¼ˆPTT æ ¼å¼ï¼‰
        content_match = re.search(r'<div id="main-content"[^>]*>(.*?)<div class="push"', html, re.DOTALL)
        
        if not content_match:
            return None
        
        content = content_match.group(1)
        
        # ç§»é™¤ HTML æ¨™ç±¤
        content = re.sub(r'<[^>]+>', '', content)
        content = re.sub(r'--\nâ€» ç™¼ä¿¡ç«™.*', '', content, flags=re.DOTALL)
        content = content.strip()
        
        return content
    
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è§£æ {url}: {e}", file=sys.stderr)
        return None

def extract_candidate_info(title, content):
    """å¾æ–‡ç« æå–å€™é¸äººè³‡è¨Š"""
    
    if not content:
        return None
    
    # æå–æŠ€èƒ½ï¼ˆé—œéµå­—ï¼‰
    skills = []
    skill_keywords = [
        'python', 'java', 'javascript', 'typescript', 'c\\+\\+', 'c#', 'go', 'rust',
        'react', 'vue', 'angular', 'node\\.js', 'django', 'flask',
        'sql', 'mysql', 'postgresql', 'mongodb', 'redis',
        'aws', 'gcp', 'azure', 'docker', 'kubernetes', 'k8s',
        'machine learning', 'ml', 'deep learning', 'ai', 'tensorflow', 'pytorch',
        'git', 'linux', 'ci/cd', 'devops'
    ]
    
    content_lower = content.lower()
    for skill in skill_keywords:
        if re.search(r'\b' + skill + r'\b', content_lower):
            skills.append(skill.upper() if len(skill) <= 3 else skill.title())
    
    # æå–ç¶“é©—å¹´è³‡
    exp_match = re.search(r'(\d+)\s*å¹´', content)
    years_of_experience = int(exp_match.group(1)) if exp_match else 0
    
    # æå–è¯çµ¡æ–¹å¼
    email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', content)
    email = email_match.group(0) if email_match else None
    
    # æå– LINE ID
    line_match = re.search(r'LINE[ï¼š:\s]*([a-zA-Z0-9_\-]+)', content, re.IGNORECASE)
    line_id = line_match.group(1) if line_match else None
    
    return {
        'title': title,
        'skills': list(set(skills)),  # å»é‡
        'years_of_experience': years_of_experience,
        'email': email,
        'line_id': line_id,
        'source': 'ptt_soft_job'
    }

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='PTT Soft_Job çˆ¬èŸ²')
    parser.add_argument('--keywords', required=True, help='æœå°‹é—œéµå­—')
    parser.add_argument('--pages', type=int, default=3, help='çˆ¬å–é æ•¸')
    parser.add_argument('--output', default=None, help='è¼¸å‡ºæª”æ¡ˆ')
    
    args = parser.parse_args()
    
    # çˆ¬å–æ–‡ç« åˆ—è¡¨
    articles = fetch_ptt_board("Soft_Job", args.pages)
    
    print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ", file=sys.stderr)
    
    # ç¯©é¸åŒ…å«é—œéµå­—çš„æ–‡ç« 
    keywords_list = [k.lower() for k in args.keywords.split(',')]
    filtered_articles = []
    
    for article in articles:
        title_lower = article['title'].lower()
        if any(kw in title_lower for kw in keywords_list) or any(kw in title_lower for kw in ['è‡ªä»‹', 'å¾µ', 'hiring']):
            filtered_articles.append(article)
    
    print(f"ğŸ“ ç¯©é¸å¾Œå‰© {len(filtered_articles)} ç¯‡ç›¸é—œæ–‡ç« ", file=sys.stderr)
    
    # è§£ææ–‡ç« å…§å®¹
    candidates = []
    for i, article in enumerate(filtered_articles[:10], 1):  # æœ€å¤šè§£æ 10 ç¯‡
        print(f"ğŸ“– è§£æ {i}/{min(10, len(filtered_articles))}: {article['title']}", file=sys.stderr)
        
        content = parse_article_content(article['url'])
        if content:
            info = extract_candidate_info(article['title'], content)
            if info and info['skills']:  # è‡³å°‘è¦æœ‰æŠ€èƒ½
                info['ptt_url'] = article['url']
                candidates.append(info)
        
        time.sleep(2)  # é¿å…éå¿«è¢«æ“‹
    
    # è¼¸å‡º
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(candidates, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ çµæœå·²å­˜è‡³ {args.output}", file=sys.stderr)
    else:
        print(json.dumps(candidates, f, ensure_ascii=False, indent=2))
    
    print(f"\nâœ… å®Œæˆï¼š{len(candidates)} ä½å€™é¸äºº", file=sys.stderr)

if __name__ == '__main__':
    main()
