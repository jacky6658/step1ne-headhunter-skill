#!/usr/bin/env python3
"""
å…¬å¸å®˜ç¶²çˆ¬èŸ²ç³»çµ±
å¾å…¬å¸å®˜ç¶² /team, /about ç­‰é é¢æå–å“¡å·¥è³‡è¨Š
"""
import json
import sys
import re
from urllib.parse import urljoin, urlparse

def get_company_website(company_name):
    """
    æœå°‹å…¬å¸å®˜ç¶²
    """
    # å¯¦éš›ä½¿ç”¨æ™‚å‘¼å« web_search
    # results = web_search(query=f"{company_name} å®˜ç¶²", count=3)
    
    # å¾æœå°‹çµæœä¸­æ‰¾å®˜ç¶²
    # for result in results:
    #     url = result.get('url', '')
    #     if is_company_website(url, company_name):
    #         return url
    
    return None

def is_company_website(url, company_name):
    """åˆ¤æ–·æ˜¯å¦ç‚ºå…¬å¸å®˜ç¶²"""
    domain = urlparse(url).netloc.lower()
    company_lower = company_name.lower().replace(" ", "")
    
    # ç°¡å–®åˆ¤æ–·ï¼šdomain åŒ…å«å…¬å¸åç¨±
    return company_lower in domain.replace("www.", "").replace("-", "").replace("_", "")

def find_team_pages(base_url):
    """
    æ‰¾å‡ºå…¬å¸å®˜ç¶²ä¸­å¯èƒ½çš„åœ˜éšŠ/é—œæ–¼é é¢
    """
    common_paths = [
        "/team",
        "/about",
        "/about-us",
        "/people",
        "/our-team",
        "/leadership",
        "/management",
        "/staff",
        "/employees",
        "/contact",
        "/é—œæ–¼æˆ‘å€‘",
        "/åœ˜éšŠ",
        "/äººå“¡"
    ]
    
    pages_to_crawl = []
    
    for path in common_paths:
        url = urljoin(base_url, path)
        pages_to_crawl.append(url)
    
    return pages_to_crawl

def extract_employee_info(page_content):
    """
    å¾é é¢å…§å®¹ä¸­æå–å“¡å·¥è³‡è¨Š
    """
    employees = []
    
    # æ–¹æ¡ˆ 1: æ­£å‰‡è¡¨é”å¼æå–
    # å¸¸è¦‹æ ¼å¼ï¼š
    # - å§“å + è·ä½ + Email
    # - <div class="team-member">...</div>
    
    # Email æ¨¡å¼
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails = re.findall(email_pattern, page_content)
    
    # é›»è©±æ¨¡å¼
    phone_pattern = r'0\d{1,2}[-\s]?\d{3,4}[-\s]?\d{4}'
    phones = re.findall(phone_pattern, page_content)
    
    # å§“åæ¨¡å¼ï¼ˆä¸­æ–‡å§“åï¼š2-4 å€‹ä¸­æ–‡å­—ï¼‰
    # é€™å€‹æ¯”è¼ƒè¤‡é›œï¼Œéœ€è¦ä¸Šä¸‹æ–‡åˆ¤æ–·
    
    # æ–¹æ¡ˆ 2: HTML çµæ§‹åŒ–è§£æ
    # å¦‚æœé é¢æœ‰çµæ§‹åŒ–çš„ team member å€å¡Š
    # å¯ä»¥ç”¨ BeautifulSoup è§£æ
    
    # æš«æ™‚è¿”å›æå–åˆ°çš„ emails å’Œ phones
    for email in emails:
        employees.append({
            "email": email,
            "phone": None,
            "name": None,
            "title": None
        })
    
    return employees

def crawl_company_website(company_name, base_url=None):
    """
    çˆ¬å–å…¬å¸å®˜ç¶²ï¼Œæå–å“¡å·¥è³‡è¨Š
    """
    print(f"ğŸ•·ï¸  çˆ¬å–å…¬å¸å®˜ç¶²ï¼š{company_name}", file=sys.stderr)
    
    # Step 1: æ‰¾åˆ°å…¬å¸å®˜ç¶²
    if not base_url:
        print(f"   ğŸ” æœå°‹å®˜ç¶²...", file=sys.stderr)
        base_url = get_company_website(company_name)
        
        if not base_url:
            print(f"   âŒ æ‰¾ä¸åˆ°å®˜ç¶²", file=sys.stderr)
            return {
                "company": company_name,
                "base_url": None,
                "success": False,
                "error": "æ‰¾ä¸åˆ°å®˜ç¶²"
            }
        
        print(f"   âœ… å®˜ç¶²ï¼š{base_url}", file=sys.stderr)
    
    # Step 2: æ‰¾å‡ºå¯èƒ½çš„ team/about é é¢
    print(f"   ğŸ“„ å°‹æ‰¾åœ˜éšŠé é¢...", file=sys.stderr)
    team_pages = find_team_pages(base_url)
    
    # Step 3: çˆ¬å–æ¯å€‹é é¢
    all_employees = []
    crawled_pages = []
    
    for page_url in team_pages:
        print(f"   ğŸ”— {page_url}", file=sys.stderr)
        
        try:
            # å¯¦éš›ä½¿ç”¨æ™‚å‘¼å« web_fetch
            # page_content = web_fetch(page_url)
            
            # æå–å“¡å·¥è³‡è¨Š
            # employees = extract_employee_info(page_content)
            
            # all_employees.extend(employees)
            
            crawled_pages.append({
                "url": page_url,
                "success": True,
                "employees_found": 0  # len(employees)
            })
            
        except Exception as e:
            crawled_pages.append({
                "url": page_url,
                "success": False,
                "error": str(e)
            })
    
    # å»é‡
    unique_employees = []
    seen_emails = set()
    
    for emp in all_employees:
        email = emp.get("email")
        if email and email not in seen_emails:
            unique_employees.append(emp)
            seen_emails.add(email)
    
    print(f"   ğŸ“Š æ‰¾åˆ° {len(unique_employees)} ä½å“¡å·¥è³‡è¨Š", file=sys.stderr)
    
    return {
        "company": company_name,
        "base_url": base_url,
        "success": True,
        "crawled_pages": crawled_pages,
        "employees": unique_employees,
        "total_employees": len(unique_employees)
    }

def batch_crawl_companies(companies):
    """æ‰¹é‡çˆ¬å–å¤šå®¶å…¬å¸"""
    results = []
    
    for i, company in enumerate(companies, 1):
        print(f"\n[{i}/{len(companies)}]", file=sys.stderr)
        
        if isinstance(company, str):
            result = crawl_company_website(company)
        elif isinstance(company, dict):
            result = crawl_company_website(
                company.get("name"),
                company.get("website")
            )
        
        results.append(result)
    
    # çµ±è¨ˆ
    success_count = sum(1 for r in results if r["success"])
    total_employees = sum(r.get("total_employees", 0) for r in results)
    
    print(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆ", file=sys.stderr)
    print(f"   ç¸½è¨ˆï¼š{len(companies)} å®¶å…¬å¸", file=sys.stderr)
    print(f"   æˆåŠŸï¼š{success_count} å®¶ï¼ˆ{success_count/len(companies)*100:.1f}%ï¼‰", file=sys.stderr)
    print(f"   å“¡å·¥è³‡è¨Šï¼š{total_employees} ä½", file=sys.stderr)
    
    return results

if __name__ == "__main__":
    # æ¸¬è©¦
    test_companies = [
        "å°ç©é›»",
        "è¯ç™¼ç§‘",
        "é´»æµ·"
    ]
    
    if len(sys.argv) > 1:
        # å¾å‘½ä»¤åˆ—åƒæ•¸è®€å–
        test_companies = sys.argv[1:]
    
    results = batch_crawl_companies(test_companies)
    
    print(json.dumps(results, ensure_ascii=False, indent=2))
