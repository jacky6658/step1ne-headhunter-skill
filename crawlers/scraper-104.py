#!/usr/bin/env python3
"""
104äººåŠ›éŠ€è¡Œè·ç¼ºçˆ¬èŸ²
ç”¨é€”ï¼šæœå°‹è·ç¼ºï¼Œæå–å…¬å¸å’Œè·ä½è³‡è¨Š
"""

import subprocess
import json
import re
import sys
from datetime import datetime

def run_browser_command(cmd):
    """åŸ·è¡Œ agent-browser æŒ‡ä»¤"""
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return result.stdout

def search_104_jobs(keyword, max_results=20):
    """æœå°‹ 104 è·ç¼º"""
    # Debug è¨Šæ¯æ”¹ç‚ºå¯«å…¥æ—¥èªŒï¼ˆä¸è¼¸å‡ºåˆ° stdout/stderrï¼‰
    with open(f"/tmp/104-scraper-debug.log", 'a', encoding='utf-8') as log:
        log.write(f"[{datetime.now()}] ğŸ” æœå°‹: {keyword}\n")
    
    # é–‹å•Ÿæœå°‹é é¢
    url = f"https://www.104.com.tw/jobs/search/?keyword={keyword}"
    run_browser_command(f'agent-browser open "{url}"')
    
    # ç­‰å¾…è¼‰å…¥
    run_browser_command('agent-browser wait --load networkidle')
    
    # å–å¾—å¿«ç…§
    snapshot_json = run_browser_command('agent-browser snapshot -i --json')
    
    try:
        data = json.loads(snapshot_json)
        refs = data.get('data', {}).get('refs', {})
    except:
        print("âŒ ç„¡æ³•è§£æçµæœ")
        return []
    
    # è§£æè·ç¼º
    jobs = []
    current_job = {}
    
    for ref_id, ref_data in refs.items():
        name = ref_data.get('name', '')
        role = ref_data.get('role', '')
        href = ref_data.get('href', '')
        
        if role != 'link':
            continue
            
        # è·ä½åç¨±ï¼ˆåŒ…å« Engineer, å·¥ç¨‹å¸«, Developer ç­‰ï¼‰
        if re.search(r'Engineer|å·¥ç¨‹å¸«|Developer|Backend|Frontend|Full.?Stack|Manager|ä¸»ç®¡', name, re.I):
            if current_job:
                jobs.append(current_job)
            # æå–çœŸå¯¦çš„è·ç¼º URL
            job_url = href if href.startswith('http') else f"https://www.104.com.tw{href}"
            current_job = {'title': name, 'ref': ref_id, 'url': job_url}
            
        # å…¬å¸åç¨±ï¼ˆé€šå¸¸åœ¨è·ä½å¾Œé¢ï¼‰
        elif current_job and 'company' not in current_job:
            if 'è‚¡ä»½æœ‰é™å…¬å¸' in name or 'æœ‰é™å…¬å¸' in name or 'ç§‘æŠ€' in name:
                current_job['company'] = name
                
        # ç”¢æ¥­
        elif current_job and 'industry' not in current_job:
            if 'æ¥­' in name and len(name) < 20:
                current_job['industry'] = name
                
        # åœ°é»
        elif current_job and 'location' not in current_job:
            if re.match(r'^(å°åŒ—|æ–°åŒ—|æ¡ƒåœ’|æ–°ç«¹|å°ä¸­|å°å—|é«˜é›„)', name):
                current_job['location'] = name
                
        # è–ªè³‡
        elif current_job and 'salary' not in current_job:
            if 'æœˆè–ª' in name or 'å¹´è–ª' in name or 'å¾…é‡' in name:
                current_job['salary'] = name
                
        # ç¶“é©—
        elif current_job and 'experience' not in current_job:
            if 'å¹´ä»¥ä¸Š' in name or 'ç¶“æ­·ä¸æ‹˜' in name:
                current_job['experience'] = name
        
        if len(jobs) >= max_results:
            break
    
    # åŠ å…¥æœ€å¾Œä¸€å€‹
    if current_job and len(jobs) < max_results:
        jobs.append(current_job)
    
    # é—œé–‰ç€è¦½å™¨
    run_browser_command('agent-browser close')
    
    return jobs

def main():
    keyword = sys.argv[1] if len(sys.argv) > 1 else "backend engineer"
    max_results = int(sys.argv[2]) if len(sys.argv) > 2 else 20
    
    jobs = search_104_jobs(keyword, max_results)
    
    # è½‰æ›ç‚ºç¬¦åˆ bd-automation.sh æœŸæœ›çš„æ ¼å¼
    formatted_jobs = []
    for job in jobs:
        formatted_jobs.append({
            "company": job.get('company', 'N/A'),
            "job_title": job.get('title', 'N/A'),
            "location": job.get('location', 'N/A'),
            "salary": job.get('salary', 'N/A'),
            "url": job.get('url', f"https://www.104.com.tw/job/{job.get('ref', '')}")
        })
    
    # è¼¸å‡º JSON åˆ°æ¨™æº–è¼¸å‡ºï¼ˆä¾› bd-automation.sh ä½¿ç”¨ï¼‰
    print(json.dumps(formatted_jobs, ensure_ascii=False, indent=2))
    
    # åŒæ™‚å­˜ä¸€ä»½åˆ° /tmp ä¾›æŸ¥çœ‹ï¼ˆdebug ç”¨ï¼‰
    output_file = f"/tmp/104-jobs-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(formatted_jobs, f, ensure_ascii=False, indent=2)
    
    # Debug info å¯«å…¥æ—¥èªŒæª”ï¼ˆä¸è¼¸å‡ºåˆ° stderrï¼Œé¿å…å¹²æ“¾ bash è…³æœ¬çš„ JSON è§£æï¼‰
    log_file = f"/tmp/104-scraper-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log"
    with open(log_file, 'w', encoding='utf-8') as log:
        log.write(f"ğŸ“Š æ‰¾åˆ° {len(jobs)} å€‹è·ç¼º\n")
        log.write(f"âœ… JSON çµæœå·²è¼¸å‡ºåˆ°æ¨™æº–è¼¸å‡º\n")
        log.write(f"âœ… å‚™ä»½å·²å­˜è‡³: {output_file}\n")

if __name__ == "__main__":
    main()
